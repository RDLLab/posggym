---
autogenerated:
title: Driving
firstpage:
---

# Driving

```{figure} ../../_static/videos/grid_world/driving.gif
:width: 200px
:name: driving
```

This environment is part of the <a href='..'>Grid World environments</a>. Please read that page first for general information.

|   |   |
|---|---|
| Possible Agents | ('0', '1') |
| Action Spaces | {'0': Discrete(5), '1': Discrete(5)} |
| Observation Spaces | {'0': Tuple(Tuple(Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4)), Discrete(4), Tuple(Discrete(14), Discrete(14)), Discrete(2), Discrete(2)), '1': Tuple(Tuple(Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4), Discrete(4)), Discrete(4), Tuple(Discrete(14), Discrete(14)), Discrete(2), Discrete(2))} |
| Symmetric | True |
| Import | `posggym.make("Driving-v0")` |


The Driving Grid World Environment.

A general-sum 2D grid world problem involving multiple agents. Each agent
controls a vehicle and is tasked with driving the vehicle from it's start
location to a destination location while avoiding crashing into obstacles
or other vehicles.

This environment requires each agent to navigate in the world while also
taking care to avoid crashing into other agents. The dynamics and
observations of the environment are such that avoiding collisions requires
some planning in order for the vehicle to brake in time or maintain a good
speed. Depending on the grid layout, the environment will require agents to
reason about and possibly coordinate with the other vehicles.

Possible Agents
---------------
The environment supports two or more agents, depending on the grid. It is possible
for some agents to finish the episode before other agents by either crashing or
reaching their destination, and so not all agents are guaranteed to be active at
the same time. All agents will be active at the start of the episode however.

State Space
-----------
Each state is made up of the state of each vehicle, which in turn is defined by:

- the `(x, y)` coordinates (x=column, y=row, with origin at the top-left square of
  the grid) of the vehicle,
- the direction the vehicle is facing `NORTH=0`, `EAST=1`, `SOUTH=2`, `WEST=3`,
- the speed of the vehicle: `REVERSE=0`, `STOPPED=1`, `FORWARD_SLOW=2`,
  `FORWARD_FAST=2`,
- the `(x, y)` coordinate of the vehicles destination
- whether the vehicle has reached it's destination or not: `1` or `0`
- whether the vehicle has crashed or not: `1` or `0`
- the minimum distance to the destination achieved by the vehicle in the current
  episode.

Action Space
------------
Each agent has 5 actions: `DO_NOTHING=0`, `ACCELERATE=1`, `DECELERATE=2`,
`TURN_RIGHT=3`, `TURN_LEFT=4`

Observation Space
-----------------
Each agent observes the cells in their local area, as well as their current speed,
their destination location, whether they've reached their destination, and whether
they've crashed. The size of the local area observed is controlled by the `obs_dims`
parameter (default = `(3, 1, 1)`, 3 cells in front, one cell behind, and 1 cell each
side, giving a observation size of 5x3). For each cell in the observed area the
agent observes whether the cell contains a `VEHICLE=0`, `WALL=1`, `EMPTY=2`, or it's
`DESTINATION=3`.

All together each agent's observation is tuple of the form:

    ((local obs), speed, destination coord, destination reached, crashed)

Rewards
-------
All agents receive a penalty of `0.0` for each step. They receive a penalty of
`-1.0` for crashing (i.e. hitting another vehicle), and `-0.05` for moving into a
wall. A reward of `1.0` is given if the agent reaches it's destination and a reward
of `0.05` is given if the agent makes progress towards it's destination (i.e. it
reduces it's minimum distance achieved to the destination for the episode).

If `obstacle_collision=True` then running into a wall is treated as crashing the
vehicle, and so results in a penalty of `-1.0`.

Dynamics
--------
Actions are deterministic and movement is determined by direction the vehicle is
facing and it's speed:

- Speed=0 (REVERSE) - vehicle moves one cell in the opposite direction to which it
    is facing
- Speed=1 (STOPPED) - vehicle remains in same cell
- Speed=2 (FORWARD_SLOW) - vehicle move one cell in facing direction
- Speed=3 (FORWARD_FAST) - vehicle moves two cells in facing direction

Accelerating increases speed by 1, while deceleration decreased speed by 1. If the
vehicle will hit a wall or another vehicle when moving from one cell to another then
it remains in it's current cell and it's crashed state variable is updated
appropriately.

Starting State
--------------
Each agent is randomly assigned to one of the possible starting locations on the
grid and one of the possible destination locations, with no two agents starting in
the same location or having the same destination location. The possible start and
destination locations are determined by the grid layout being used.

Episodes End
------------
Episodes end when all agents have either reached their destination or crashed. By
default a `max_episode_steps` is also set for each Driving environment. The default
value is `50` steps, but this may need to be adjusted when using larger grids (this
can be done by manually specifying a value for `max_episode_steps` when creating the
environment with `posggym.make`).

Arguments
---------

- `grid` - the grid layout to use. This can either be a string specifying one of
     the supported grids, or a custom :class:`DrivingGrid` object
     (default = `"14x14RoundAbout"`).
- `num_agents` - the number of agents in the environment (default = `2`).
- `obs_dim` - the local observation dimensions, specifying how many cells in front,
     behind, and to each side the agent observes (default = `(3, 1, 1)`, resulting
     in the agent observing a 5x3 area: 3 in front, 1 behind, 1 to each side.)
- `obstacle_collisions` -  whether running into a wall results in the agent's
     vehicle crashing and thus the agent reaching a terminal state. This can make
     the problem significantly harder (default = "False").

Available variants
------------------

The Driving environment comes with a number of pre-built grid layouts which can be
passed as an argument to `posggym.make`, to create different grids:

| Grid name         | Max number of agents | Grid size |
|-------------------|----------------------|---------- |
| `3x3`             | 2                    | 3x3       |
| `6x6`             | 6                    | 6x6       |
| `7x7Blocks`       | 4                    | 7x7       |
| `7x7CrissCross`   | 6                    | 7x7       |
| `7x7RoundAbout`   | 4                    | 7x7       |
| `14x14Blocks`     | 4                    | 14x14     |
| `14x14CrissCross` | 8                    | 14x14     |
| `14x14RoundAbout` | 4                    | 14x14     |


For example to use the Driving environment with the `7x7RoundAbout` grid and 2
agents, you would use:

```python
import posggym
env = posggym.make('Driving-v0', grid="7x7RoundAbout", num_agents=2)
```

Version History
---------------
- `v0`: Initial version

References
----------
- Adam Lerer and Alexander Peysakhovich. 2019. Learning Existing Social Conventions
via Observationally Augmented Self-Play. In Proceedings of the 2019 AAAI/ACM
Conference on AI, Ethics, and Society. 107–114.
- Kevin R. McKee, Joel Z. Leibo, Charlie Beattie, and Richard Everett. 2022.
Quantifying the Effects of Environment and Population Diversity in Multi-Agent
Reinforcement Learning. Autonomous Agents and Multi-Agent Systems 36, 1 (2022), 1–16
